{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JM0kuK3SJNKY"
      },
      "outputs": [],
      "source": [
        "### Importar Librerías\n",
        "\n",
        "import torch\n",
        "from torchvision.datasets import Food101\n",
        "from torchvision import transforms, models\n",
        "from torch.utils.data import DataLoader, Subset, Dataset, ConcatDataset\n",
        "import random\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "\"\"\"### Configuración inicial\"\"\"\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Usando dispositivo: {device}\")\n",
        "\n",
        "# Fijar semillas para reproducibilidad\n",
        "torch.manual_seed(42)\n",
        "random.seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "\"\"\"### Transformaciones\"\"\"\n",
        "\n",
        "# Transformación base (sin data augmentation)\n",
        "base_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                        std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Transformación con data augmentation para entrenamiento\n",
        "augment_transform = transforms.Compose([\n",
        "    transforms.Resize((256, 256)),\n",
        "    transforms.RandomResizedCrop(224, scale=(0.7, 1.0)),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.RandomRotation(degrees=15),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                        std=[0.229, 0.224, 0.225]),\n",
        "    transforms.RandomErasing(p=0.3, scale=(0.02, 0.2), ratio=(0.3, 3.3))\n",
        "])\n",
        "\n",
        "\"\"\"### Cargar datasets base\"\"\"\n",
        "\n",
        "# Dataset para visualización\n",
        "viz_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "print(\"Cargando datasets...\")\n",
        "train_dataset = Food101(root='.', download=True, split='train', transform=viz_transform)\n",
        "test_dataset = Food101(root='.', download=True, split='test', transform=viz_transform)\n",
        "\n",
        "# Obtener índices de hot dogs\n",
        "hot_dog_index = train_dataset.classes.index('hot_dog')\n",
        "print(f\"Índice de 'hot_dog': {hot_dog_index}\")\n",
        "\n",
        "# Obtener todos los índices\n",
        "targets = train_dataset._labels\n",
        "hotdog_indices = [i for i, label in enumerate(targets) if label == hot_dog_index]\n",
        "not_hotdog_indices = [i for i, label in enumerate(targets) if label != hot_dog_index]\n",
        "\n",
        "test_targets = test_dataset._labels\n",
        "test_hotdog_indices = [i for i, label in enumerate(test_targets) if label == hot_dog_index]\n",
        "test_not_hotdog_indices = [i for i, label in enumerate(test_targets) if label != hot_dog_index]\n",
        "\n",
        "print(f\"\\nDatos disponibles:\")\n",
        "print(f\"Train - Hot dogs: {len(hotdog_indices)}, No hot dogs: {len(not_hotdog_indices)}\")\n",
        "print(f\"Test - Hot dogs: {len(test_hotdog_indices)}, No hot dogs: {len(test_not_hotdog_indices)}\")\n",
        "\n",
        "\"\"\"### Dataset binario personalizado\"\"\"\n",
        "\n",
        "class BinaryFoodDataset(Dataset):\n",
        "    def __init__(self, original_dataset, indices, labels, transform=None):\n",
        "        self.original_dataset = original_dataset\n",
        "        self.indices = indices\n",
        "        self.labels = labels\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.indices)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        original_idx = self.indices[idx]\n",
        "\n",
        "        # Obtener imagen PIL del dataset original\n",
        "        image, _ = self.original_dataset[original_idx]\n",
        "\n",
        "        # Convertir tensor a PIL si es necesario\n",
        "        if torch.is_tensor(image):\n",
        "            # Convertir de tensor a PIL\n",
        "            image = transforms.ToPILImage()(image)\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        else:\n",
        "            image = base_transform(image)\n",
        "\n",
        "        label = torch.tensor([self.labels[idx]], dtype=torch.float32)\n",
        "        return image, label\n",
        "\n",
        "\"\"\"### Dataset con repetición para data augmentation\"\"\"\n",
        "\n",
        "class AugmentedHotdogDataset(Dataset):\n",
        "    def __init__(self, original_dataset, hotdog_indices, target_size, transform):\n",
        "        self.original_dataset = original_dataset\n",
        "        self.hotdog_indices = hotdog_indices\n",
        "        self.target_size = target_size\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.target_size\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Elegir un hot dog al azar\n",
        "        original_idx = random.choice(self.hotdog_indices)\n",
        "        image, _ = self.original_dataset[original_idx]\n",
        "\n",
        "        # Convertir tensor a PIL si es necesario\n",
        "        if torch.is_tensor(image):\n",
        "            image = transforms.ToPILImage()(image)\n",
        "\n",
        "        image = self.transform(image)\n",
        "        label = torch.tensor([1.0], dtype=torch.float32)\n",
        "        return image, label\n",
        "\n",
        "\"\"\"### Preparar datasets de test (diferentes para cada experimento)\"\"\"\n",
        "\n",
        "def create_balanced_test_loader():\n",
        "    \"\"\"Crea el test loader balanceado para el Experimento 1\"\"\"\n",
        "    random.seed(42)\n",
        "    test_balanced_not_hotdog = random.sample(test_not_hotdog_indices, len(test_hotdog_indices))\n",
        "    test_all_indices = test_hotdog_indices + test_balanced_not_hotdog\n",
        "    test_labels = [1.0] * len(test_hotdog_indices) + [0.0] * len(test_balanced_not_hotdog)\n",
        "\n",
        "    test_binary_ds = BinaryFoodDataset(\n",
        "        test_dataset,\n",
        "        test_all_indices,\n",
        "        test_labels,\n",
        "        transform=base_transform\n",
        "    )\n",
        "\n",
        "    test_loader = DataLoader(test_binary_ds, batch_size=32, shuffle=False, num_workers=2)\n",
        "    print(f\"Test set balanceado: {len(test_binary_ds)} muestras ({len(test_hotdog_indices)} hot dogs, {len(test_balanced_not_hotdog)} no hot dogs)\")\n",
        "    return test_loader\n",
        "\n",
        "def create_full_test_loader():\n",
        "    \"\"\"Crea el test loader completo para los Experimentos 2 y 3\"\"\"\n",
        "    # Usar TODO el conjunto de test\n",
        "    test_all_indices = test_hotdog_indices + test_not_hotdog_indices\n",
        "    test_labels = [1.0] * len(test_hotdog_indices) + [0.0] * len(test_not_hotdog_indices)\n",
        "\n",
        "    test_binary_ds = BinaryFoodDataset(\n",
        "        test_dataset,\n",
        "        test_all_indices,\n",
        "        test_labels,\n",
        "        transform=base_transform\n",
        "    )\n",
        "\n",
        "    test_loader = DataLoader(test_binary_ds, batch_size=32, shuffle=False, num_workers=2)\n",
        "    print(f\"Test set completo: {len(test_binary_ds)} muestras ({len(test_hotdog_indices)} hot dogs, {len(test_not_hotdog_indices)} no hot dogs)\")\n",
        "    return test_loader\n",
        "\n",
        "\"\"\"### Modelo CNN personalizado\"\"\"\n",
        "\n",
        "class CNNBinary(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(3, 16, 3, padding=1), nn.ReLU(), nn.MaxPool2d(2),\n",
        "            nn.Conv2d(16, 32, 3, padding=1), nn.ReLU(), nn.MaxPool2d(2),\n",
        "            nn.Conv2d(32, 64, 3, padding=1), nn.ReLU(), nn.MaxPool2d(2),\n",
        "        )\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(64*28*28, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(128, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.classifier(self.features(x))\n",
        "\n",
        "\"\"\"### Función de entrenamiento común\"\"\"\n",
        "\n",
        "def train_model(model, train_loader, test_loader, model_name, n_epochs=5):\n",
        "    print(f\"\\n{'='*50}\")\n",
        "    print(f\"ENTRENANDO: {model_name}\")\n",
        "    print(f\"{'='*50}\")\n",
        "\n",
        "    # Limpiar memoria GPU antes de cada entrenamiento\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    # Reinicializar completamente el modelo\n",
        "    model.apply(lambda m: m.reset_parameters() if hasattr(m, 'reset_parameters') else None)\n",
        "\n",
        "    model = model.to(device)\n",
        "    criterion = nn.BCELoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-4)\n",
        "\n",
        "    best_test_acc = 0.0\n",
        "    results = []\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    for epoch in range(n_epochs):\n",
        "        # ——————— ENTRENAMIENTO ———————\n",
        "        model.train()\n",
        "        total_loss = 0.0\n",
        "        correct_train = 0\n",
        "        total_train = 0\n",
        "\n",
        "        for batch_idx, (images, labels) in enumerate(train_loader):\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item() * labels.size(0)\n",
        "            predicted = (outputs > 0.5).float()\n",
        "            correct_train += (predicted == labels).sum().item()\n",
        "            total_train += labels.size(0)\n",
        "\n",
        "        avg_loss = total_loss / total_train\n",
        "        train_acc = 100.0 * correct_train / total_train\n",
        "\n",
        "        # ——————— EVALUACIÓN ———————\n",
        "        model.eval()\n",
        "        correct_test = 0\n",
        "        total_test = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for images, labels in test_loader:\n",
        "                images, labels = images.to(device), labels.to(device)\n",
        "                outputs = model(images)\n",
        "                predicted = (outputs > 0.5).float()\n",
        "                correct_test += (predicted == labels).sum().item()\n",
        "                total_test += labels.size(0)\n",
        "\n",
        "        test_acc = 100.0 * correct_test / total_test\n",
        "\n",
        "        if test_acc > best_test_acc:\n",
        "            best_test_acc = test_acc\n",
        "\n",
        "        results.append({\n",
        "            'epoch': epoch + 1,\n",
        "            'train_loss': avg_loss,\n",
        "            'train_acc': train_acc,\n",
        "            'test_acc': test_acc\n",
        "        })\n",
        "\n",
        "        print(f'Epoch {epoch+1}/{n_epochs}: Loss: {avg_loss:.4f}, Train Acc: {train_acc:.2f}%, Test Acc: {test_acc:.2f}%')\n",
        "\n",
        "    training_time = time.time() - start_time\n",
        "    print(f\"\\nTiempo de entrenamiento: {training_time:.2f} segundos\")\n",
        "    print(f\"Mejor Test Accuracy: {best_test_acc:.2f}%\")\n",
        "\n",
        "    return model, results, best_test_acc\n",
        "\n",
        "\"\"\"### Función de evaluación detallada\"\"\"\n",
        "\n",
        "def evaluate_detailed(model, test_loader, model_name):\n",
        "    model.eval()\n",
        "    all_predictions = []\n",
        "    all_labels = []\n",
        "    all_probabilities = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            predicted = (outputs > 0.5).float()\n",
        "\n",
        "            all_predictions.extend(predicted.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "            all_probabilities.extend(outputs.cpu().numpy())\n",
        "\n",
        "    all_predictions = np.array(all_predictions).flatten()\n",
        "    all_labels = np.array(all_labels).flatten()\n",
        "    all_probabilities = np.array(all_probabilities).flatten()\n",
        "\n",
        "    # Calcular métricas\n",
        "    tp = np.sum((all_predictions == 1) & (all_labels == 1))\n",
        "    tn = np.sum((all_predictions == 0) & (all_labels == 0))\n",
        "    fp = np.sum((all_predictions == 1) & (all_labels == 0))\n",
        "    fn = np.sum((all_predictions == 0) & (all_labels == 1))\n",
        "\n",
        "    accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
        "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
        "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
        "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
        "\n",
        "    print(f\"\\n{model_name} - Resultados detallados:\")\n",
        "    print(f\"Accuracy: {accuracy*100:.2f}%\")\n",
        "    print(f\"Precision: {precision:.3f}\")\n",
        "    print(f\"Recall: {recall:.3f}\")\n",
        "    print(f\"F1-Score: {f1:.3f}\")\n",
        "    print(f\"TP: {tp}, TN: {tn}, FP: {fp}, FN: {fn}\")\n",
        "\n",
        "    return {\n",
        "        'accuracy': accuracy,\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'f1': f1,\n",
        "        'tp': tp, 'tn': tn, 'fp': fp, 'fn': fn\n",
        "    }\n",
        "\n",
        "\"\"\"### EXPERIMENTO 1: Balanceo básico (750 muestras cada clase) - Test balanceado\"\"\"\n",
        "\n",
        "def experiment_1():\n",
        "    print(f\"\\n🔥 EXPERIMENTO 1: BALANCEO BÁSICO (750 + 750) - TEST BALANCEADO\")\n",
        "\n",
        "    # Limpiar memoria antes del experimento\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    # Usar solo 750 hot dogs - nueva semilla para este experimento\n",
        "    random.seed(42)\n",
        "    torch.manual_seed(42)\n",
        "\n",
        "    selected_hotdog_indices = random.sample(hotdog_indices, 750)\n",
        "    selected_not_hotdog_indices = random.sample(not_hotdog_indices, 750)\n",
        "\n",
        "    # Crear dataset\n",
        "    train_indices = selected_hotdog_indices + selected_not_hotdog_indices\n",
        "    train_labels = [1.0] * 750 + [0.0] * 750\n",
        "\n",
        "    train_ds = BinaryFoodDataset(\n",
        "        train_dataset,\n",
        "        train_indices,\n",
        "        train_labels,\n",
        "        transform=base_transform\n",
        "    )\n",
        "\n",
        "    train_loader = DataLoader(train_ds, batch_size=32, shuffle=True, num_workers=2)\n",
        "\n",
        "    # Crear test loader balanceado\n",
        "    test_loader = create_balanced_test_loader()\n",
        "\n",
        "    print(f\"Dataset de entrenamiento: {len(train_ds)} muestras\")\n",
        "\n",
        "    # Crear NUEVO modelo CNN desde cero\n",
        "    model1 = CNNBinary()\n",
        "    print(\"✅ Modelo CNN creado desde cero para Experimento 1\")\n",
        "\n",
        "    model1, results, best_acc = train_model(model1, train_loader, test_loader, \"CNN con Balanceo Básico\")\n",
        "\n",
        "    # Limpiar después del entrenamiento\n",
        "    del train_loader, train_ds\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    return model1, results, best_acc, test_loader\n",
        "\n",
        "\"\"\"### EXPERIMENTO 2: Data Augmentation (10,000 muestras cada clase) - Test completo\"\"\"\n",
        "\n",
        "def experiment_2():\n",
        "    print(f\"\\n🚀 EXPERIMENTO 2: DATA AUGMENTATION (10,000 + 10,000) - TEST COMPLETO\")\n",
        "\n",
        "    # Limpiar memoria antes del experimento\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    # Reinicializar semillas para este experimento\n",
        "    random.seed(123)  # Diferente semilla\n",
        "    torch.manual_seed(123)\n",
        "\n",
        "    # Dataset aumentado de hot dogs\n",
        "    hotdog_augmented_ds = AugmentedHotdogDataset(\n",
        "        train_dataset,\n",
        "        hotdog_indices,\n",
        "        target_size=10000,\n",
        "        transform=augment_transform\n",
        "    )\n",
        "\n",
        "    # Dataset de no-hot-dogs (10,000 muestras)\n",
        "    selected_not_hotdog_indices = random.sample(not_hotdog_indices, 10000)\n",
        "    not_hotdog_labels = [0.0] * 10000\n",
        "\n",
        "    not_hotdog_ds = BinaryFoodDataset(\n",
        "        train_dataset,\n",
        "        selected_not_hotdog_indices,\n",
        "        not_hotdog_labels,\n",
        "        transform=augment_transform\n",
        "    )\n",
        "\n",
        "    # Combinar datasets\n",
        "    combined_ds = ConcatDataset([hotdog_augmented_ds, not_hotdog_ds])\n",
        "    train_loader = DataLoader(combined_ds, batch_size=32, shuffle=True, num_workers=2)\n",
        "\n",
        "    # Crear test loader completo\n",
        "    test_loader = create_full_test_loader()\n",
        "\n",
        "    print(f\"Dataset de entrenamiento: {len(combined_ds)} muestras\")\n",
        "\n",
        "    # Crear NUEVO modelo CNN desde cero\n",
        "    model2 = CNNBinary()\n",
        "    print(\"✅ Modelo CNN creado desde cero para Experimento 2\")\n",
        "\n",
        "    model2, results, best_acc = train_model(model2, train_loader, test_loader, \"CNN con Data Augmentation\", n_epochs=7)\n",
        "\n",
        "    # Limpiar después del entrenamiento\n",
        "    del train_loader, combined_ds, hotdog_augmented_ds, not_hotdog_ds\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    return model2, results, best_acc, test_loader\n",
        "\n",
        "\"\"\"### EXPERIMENTO 3: ResNet18 Preentrenado - Test completo\"\"\"\n",
        "\n",
        "def experiment_3():\n",
        "    print(f\"\\n🎯 EXPERIMENTO 3: RESNET18 PREENTRENADO (10,000 + 10,000) - TEST COMPLETO\")\n",
        "\n",
        "    # Limpiar memoria antes del experimento\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    # Reinicializar semillas para este experimento\n",
        "    random.seed(456)  # Diferente semilla\n",
        "    torch.manual_seed(456)\n",
        "\n",
        "    # Dataset aumentado de hot dogs\n",
        "    hotdog_augmented_ds = AugmentedHotdogDataset(\n",
        "        train_dataset,\n",
        "        hotdog_indices,\n",
        "        target_size=10000,\n",
        "        transform=augment_transform\n",
        "    )\n",
        "\n",
        "    selected_not_hotdog_indices = random.sample(not_hotdog_indices, 10000)\n",
        "    not_hotdog_labels = [0.0] * 10000\n",
        "\n",
        "    not_hotdog_ds = BinaryFoodDataset(\n",
        "        train_dataset,\n",
        "        selected_not_hotdog_indices,\n",
        "        not_hotdog_labels,\n",
        "        transform=augment_transform\n",
        "    )\n",
        "\n",
        "    combined_ds = ConcatDataset([hotdog_augmented_ds, not_hotdog_ds])\n",
        "    train_loader = DataLoader(combined_ds, batch_size=32, shuffle=True, num_workers=2)\n",
        "\n",
        "    # Crear test loader completo\n",
        "    test_loader = create_full_test_loader()\n",
        "\n",
        "    print(f\"Dataset de entrenamiento: {len(combined_ds)} muestras\")\n",
        "\n",
        "    # Crear NUEVO modelo ResNet18 desde cero\n",
        "    model3 = models.resnet18(pretrained=True)\n",
        "    print(\"✅ Modelo ResNet18 cargado NUEVO desde pretrained para Experimento 3\")\n",
        "\n",
        "    # Fine-tuning: descongelar últimas capas\n",
        "    for param in model3.parameters():\n",
        "        param.requires_grad = False\n",
        "\n",
        "    # Descongelar las últimas capas\n",
        "    for param in model3.layer4.parameters():\n",
        "        param.requires_grad = True\n",
        "    for param in model3.avgpool.parameters():\n",
        "        param.requires_grad = True\n",
        "\n",
        "    # Reemplazar la última capa\n",
        "    num_features = model3.fc.in_features\n",
        "    model3.fc = nn.Sequential(\n",
        "        nn.Dropout(0.5),\n",
        "        nn.Linear(num_features, 1),\n",
        "        nn.Sigmoid()\n",
        "    )\n",
        "\n",
        "    model3, results, best_acc = train_model(model3, train_loader, test_loader, \"ResNet18 Preentrenado\", n_epochs=7)\n",
        "\n",
        "    # Limpiar después del entrenamiento\n",
        "    del train_loader, combined_ds, hotdog_augmented_ds, not_hotdog_ds\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    return model3, results, best_acc, test_loader\n",
        "\n",
        "\"\"\"### Función principal para ejecutar todos los experimentos\"\"\"\n",
        "\n",
        "def run_all_experiments():\n",
        "    results_summary = []\n",
        "\n",
        "    print(\"🔥🚀🎯 INICIANDO LOS 3 EXPERIMENTOS 🎯🚀🔥\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # Experimento 1 - Test balanceado\n",
        "    model1, results1, best_acc1, test_loader1 = experiment_1()\n",
        "    detailed1 = evaluate_detailed(model1, test_loader1, \"Experimento 1\")\n",
        "    results_summary.append((\"Balanceo Básico (750+750) - Test Balanceado\", best_acc1, detailed1))\n",
        "\n",
        "    # Experimento 2 - Test completo\n",
        "    model2, results2, best_acc2, test_loader2 = experiment_2()\n",
        "    detailed2 = evaluate_detailed(model2, test_loader2, \"Experimento 2\")\n",
        "    results_summary.append((\"Data Augmentation (10k+10k) - Test Completo\", best_acc2, detailed2))\n",
        "\n",
        "    # Experimento 3 - Test completo\n",
        "    model3, results3, best_acc3, test_loader3 = experiment_3()\n",
        "    detailed3 = evaluate_detailed(model3, test_loader3, \"Experimento 3\")\n",
        "    results_summary.append((\"ResNet18 Preentrenado (10k+10k) - Test Completo\", best_acc3, detailed3))\n",
        "\n",
        "    # Resumen final\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(\"🏆 RESUMEN FINAL DE TODOS LOS EXPERIMENTOS 🏆\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    for i, (name, best_acc, detailed) in enumerate(results_summary, 1):\n",
        "        print(f\"\\n{i}. {name}:\")\n",
        "        print(f\"   Mejor Test Accuracy: {best_acc:.2f}%\")\n",
        "        print(f\"   Precision: {detailed['precision']:.3f}\")\n",
        "        print(f\"   Recall: {detailed['recall']:.3f}\")\n",
        "        print(f\"   F1-Score: {detailed['f1']:.3f}\")\n",
        "\n",
        "    # Encontrar el mejor (considerando que el Experimento 1 usa test diferente)\n",
        "    print(f\"\\n📊 NOTA IMPORTANTE:\")\n",
        "    print(f\"   - Experimento 1: Usa test balanceado (250 hot dogs + 250 no hot dogs)\")\n",
        "    print(f\"   - Experimentos 2 y 3: Usan test completo (250 hot dogs + 24,750 no hot dogs)\")\n",
        "    print(f\"   - Los resultados NO son directamente comparables debido a la diferencia en distribución\")\n",
        "\n",
        "    best_balanced = results_summary[0]  # Experimento 1\n",
        "    best_full = max(results_summary[1:], key=lambda x: x[1])  # Experimentos 2 y 3\n",
        "\n",
        "    print(f\"\\n🥇 MEJOR en Test Balanceado: {best_balanced[0]} con {best_balanced[1]:.2f}%\")\n",
        "    print(f\"🥇 MEJOR en Test Completo: {best_full[0]} con {best_full[1]:.2f}%\")\n",
        "\n",
        "    return results_summary\n",
        "\n",
        "\"\"\"### Ejecutar todos los experimentos\"\"\"\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    results = run_all_experiments()"
      ]
    }
  ]
}